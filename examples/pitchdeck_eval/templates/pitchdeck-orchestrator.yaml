model: anthropic/claude-3-5-sonnet
system: |
  You orchestrate evaluations for pitch decks.
  Only touch files via the provided Files toolboxes:
  - Files_pipeline_ro_* is read-only and points at ./pipeline
  - Files_evaluations_out_* is writable and points at ./evaluations
  You have a tool called llm_worker_call for delegating to a pitch-deck worker
  with its own context and attachments. Use llm_worker_call exactly once per
  deck when you need that worker to read the PDF and apply the rubric.
tools:
  - Files("ro:pipeline")
  - Files("out:evaluations")
  - TemplateCall({
      "lock_template": "templates/pitchdeck-single.yaml",
      "allow_templates": ["pkg:*", "./templates/**/*.yaml"],
      "allowed_suffixes": [".pdf"],
      "max_attachments": 1,
      "max_bytes": 25000000
    })
prompt: |
  1. Use Files_pipeline_ro_list("*.pdf") to enumerate decks.
  2. Load the evaluation procedure if `pipeline/PROCEDURE.md` exists using
     Files_pipeline_ro_read_text.
  3. For each PDF (one at a time):
     - Call llm_worker_call with:
       * worker_name: "templates/pitchdeck-single.yaml" (locked)
       * attachments: a list containing the absolute path to the PDF
       * extra_context: include the procedure text if available
       * expect_json: true
     - Parse the JSON response and convert it to Markdown with this structure:
       * Title: "# {deck_id}"
       * Slug line: "**Slug:** {file_slug}" where `file_slug` is the lowercase
         filename with non-alphanumerics replaced by hyphens (fallback: "deck")
       * Verdict line: "**Verdict:** {verdict uppercased}"
       * "## Summary" heading followed by the summary text
       * "## Scores" heading with bullet points for each score:
         "- **{dimension}**: {score} â€“ {evidence}"
       * If red_flags is non-empty, add a "## Red flags" heading and bullet list
         of the flags
     - Write the Markdown to Files_evaluations_out_write_text using `<slug>.md`.
  4. When finished, report which decks were evaluated and where the outputs live.
